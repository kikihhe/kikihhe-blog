---
title: 调度器工作流程
date: 2025-02-21 20:42:14
permalink: /pages/e6af11/
---
## 1. 概述

调度器的主要作用是根据特定的调度算法和调度策略将 Pod 调度到合适的 Node 节点上，是一个独立的二进制程序，启动之后会通过 Informer 监听 api-server，获取需要的 pod 然后进行调度

所谓的调度，主要分为两个步骤 ：

1. 调度过程 ：从 api-server 获取（监听）需要调度的 pod，将其调度到指定的 node，这个过程是同步的
    - 预选 ：过滤掉不满足条件的 node，将满足条件的 node 列表传入优选步骤
    - 优选 ：在满足条件的 node 中，选出一个优先级（分数）最高的 node 进行调度
2. 绑定过程 ：将调度过程的决策应用到集群中（也就是在被选定的节点上运行Pod）

调度过程和绑定过程一起被称为 **调度上下文**。

## 2. 调度上下文

在调度上下文中提供了很多扩展点，调度和绑定的所有逻辑都是这些扩展点实现的，也就是以插件的形式实现所有功能。

<img src="https://typorehwf.oss-cn-chengdu.aliyuncs.com/scheduling-framework-extensions.png" alt="调度上下文" style="zoom:80%;" />

### 2.1 调度周期

调度周期是同步运行的，主要包括以下步骤 ：

1. **Sort** ：用于对调度队列中的 Pod 进行排序，以决定先调度哪个 Pod，本质上只需要实现 `Less(p1, p2)` 方法 就可以比较两个 Pod 谁的优先级更高，因为从 api-server 获取的数据会放入调度队列，调度队列使用优先级队列来实现，优先级的比较就是 Sort 的过程。
2. **PreFilter** ：用于对 Pod 的信息进行预处理，或者检查一些 Pod 必须满足的前置条件，然后将其存入缓存中
3. **Filter** ：用来过滤掉不满足 Pod 调度条件的 Node，每一个节点都会执行所有 Filter，用来过滤掉不满足的 Nodes
4. **PostFilter** ：如果所有的 Node 都不满足 Pod 调度条件，会执行该扩展点。若开启了抢占特性，则会抢占低优先级 Pod 选中的 Node（已调度未绑定）
5. **PreScore** ：对 Score 扩展点的数据做预处理操作，然后将其放入缓存中待 Score 使用
6. **Score** ：用于为满足条件的 Node 打分
7. **NormalizeScore** ：在调度器对节点进行最终排序之前修改每一个节点的评分结果。该扩展点执行时会获取 Score 的评分结果，用户可在这里影响最终评分结果。
8. **Reserve** ：该扩展可以获取 Node 为 Pod 预留的资源，在这里可以进行判断资源是否足够
9. **Permit** ：用于阻止或者延迟 Pod 与 Node 的绑定
    - approve ：批准，当所有的 Permit 扩展都允许时，Pod 即可与 Node 绑定
    - deny ：拒绝，任何一个 Permit 扩展拒绝绑定则终止调度
    - wait ：等待，调度会保持在 Permit 阶段，直到被其他扩展 approve。超时则变为 deny 状态，Pod 进入待调度队列。

### 2.2 绑定周期

绑定周期是并发运行的，可以同步进行多个 Pod 的绑定

1. PreBind ：用于绑定前执行的逻辑
2. Bind ：绑定，该插件只有一个会执行
3. PostBind ：通知性质的扩展，绑定成功后调用。



## 3. 调度器组成

调度器想要调度 Pod 肯定要监听 api-server，所以 scheduler 中一定有 informer 对象。

```go
type Scheduler struct {
	SchedulerCache internalcache.Cache
	// 调度算法
	Algorithm ScheduleAlgorithm

	Extenders []framework.Extender
	
    // 获取下一个需要调度的 pod, 从 SchedulingQueue 中获取
	NextPod func() *framework.QueuedPodInfo

	Error func(*framework.QueuedPodInfo, error)

	StopEverything <-chan struct{}

    // 调度队列
	SchedulingQueue internalqueue.SchedulingQueue

    // 内含 informer、client、kubeConfig、extenders
	Profiles profile.Map
	
    // 客户端
	client clientset.Interface
}
```

其中最重要的是 

1. Profiles 内的 Informer ：监听器，监听 api-server 的 Pod 事件，放入调度器队列中
2. SchedulingQueue ：调度器队列，堆结构，进入该队列的 Pod 会通过 LessFunc 比较优先级。
3. NextPod ：从调度器队列中获取一个优先级最高的 Pod 

### 3.1 SchedulingQueue

调度器队列本质是一个优先级队列，根据 Pod 优先级进行排序

```go
// 优先级队列
type PriorityQueue struct {
	// ...
    
	// 用来存放等待调度的 Pod
	activeQ *heap.Heap

	// 回退队列，如果调度任务反复执行失败，则会按照 retry 次数增加等待重试时间
	// 对于调度失败的 Pod，会优先存储在 podBackoffQ 中等待重试，所以 podBackoffQ 可以理解为重试队列
	podBackoffQ *heap.Heap

	// 不可调度队列，Pod 不满足调度条件时加入
	unschedulableQ *UnschedulablePodsMap

	schedulingCycle int64

	moveRequestCycle int64

	clusterEventMap map[framework.ClusterEvent]sets.String

	closed bool

	nsLister listersv1.NamespaceLister
}
```

调度器队列中有三个堆结构 ：

1. activeQ ：存放等待调度的 Pod，Informer 拿到数据后第一个时间放在这里
2. podBackoffQ ：回退队列，如果调度任务反复执行失败，则会按照 retry 次数增加等待重试时间
3. unschedulableQ ：不可调度队列，Pod 不满足调度条件时加入

### 3.2 LessFunc

既然是堆，肯定有比较规则，这个比较规则就是 Pod 的优先级规则 ：`LessFunc`

```go
type Heap struct {
	data *data
	metricRecorder metrics.MetricRecorder
}


type data struct {

	items map[string]*heapItem

	queue []string

	keyFunc KeyFunc
    
	// 非常重要
	lessFunc lessFunc
}
```

向调度器队列中添加数据的方法 ：Add

```go
func (h *Heap) Add(obj interface{}) error {
	key, err := h.data.keyFunc(obj)
	if err != nil {
		return cache.KeyError{Obj: obj, Err: err}
	}
	if _, exists := h.data.items[key]; exists {
		h.data.items[key].obj = obj
        // 调用 Fix 进行堆结构的调整，Fix调用 up/down, up/down 使用 Less 判断优先级
		heap.Fix(h.data, h.data.items[key].index)
	} else {
		heap.Push(h.data, &itemKeyValue{key, obj})
		if h.metricRecorder != nil {
			h.metricRecorder.Inc()
		}
	}
	return nil
}

// Fix 调用 Less
func (h *data) Less(i, j int) bool {
	if i > len(h.queue) || j > len(h.queue) {
		return false
	}
	itemi, ok := h.items[h.queue[i]]
	if !ok {
		return false
	}
	itemj, ok := h.items[h.queue[j]]
	if !ok {
		return false
	}
    // Less 调用 LessFunc
	return h.lessFunc(itemi.obj, itemj.obj)
}
```

创建调度器时肯定要先指定 Pod 的比较规则，根据比较规则创建调度器队列，LessFunc 的实现：

```go
// pkg/scheduler/factory.go create()
lessFn := profiles[c.profiles[0].SchedulerName].QueueSortFunc()


// pkg/scheduler/framework/runtime/framework.go
// QueueSortFunc返回对调度队列中的pod进行排序的函数
func (f *frameworkImpl) QueueSortFunc() framework.LessFunc {
    // ...
    
	// 只能启用一个QueueSort插件
	return f.queueSortPlugins[0].Less
}
```

最终真正用于优先级队列元素优先级比较的函数是通过 QueueSort 插件来实现的，默认启用的 QueueSort 插件是 PrioritySort，PrioritySort 插件的实现核心就是 `Less()`方法 ：比较 pod.Spec 中配置的 Priority，都一样再比较时间戳

```go
// pkg/scheduler/framework/plugins/queuesort/priority_sort.go
// Less 是 activeQ 用于对 pod 进行排序的函数,它根据 pod 的优先级对 pod 进行排序
// 当优先级相同时,它使用 PodQueueInfo.timestamp 进行比较
func (pl *PrioritySort) Less(pInfo1, pInfo2 *framework.QueuedPodInfo) bool {
	p1 := corev1helpers.PodPriority(pInfo1.Pod)
	p2 := corev1helpers.PodPriority(pInfo2.Pod)
	return (p1 > p2) || (p1 == p2 && pInfo1.Timestamp.Before(pInfo2.Timestamp))
}

// k8s.io/component-helpers/scheduling/corev1/helpers.go
func PodPriority(pod *v1.Pod) int32 {
	if pod.Spec.Priority != nil {
		return *pod.Spec.Priority
	}
	return 0
}
```



### 3.3 NextPod

非常 ez 的逻辑，从调度器队列的 activeQ 中 pop 出头部数据

```go
// MakeNextPodFunc returns a function to retrieve the next pod from a given
// scheduling queue
func MakeNextPodFunc(queue SchedulingQueue) func() *framework.QueuedPodInfo {
	return func() *framework.QueuedPodInfo {
		podInfo, err := queue.Pop()
		if err == nil {
			klog.V(4).InfoS("About to try and schedule pod", "pod", klog.KObj(podInfo.Pod))
			return podInfo
		}
		klog.ErrorS(err, "Error while retrieving next pod from scheduling queue")
		return nil
	}
}
```



## 4. 调度器创建

### 4.1 scheduler.New

创建 scheduler 的调用链 ：

- main

    `cmd/kube-scheduler/scheduler.go`

- NewSchedulerCommand

    `cmd/kube-scheduler/app/server.go`

- runCommand

    `cmd/kube-scheduler/app/server.go`

- Setup

    `cmd/kube-scheduler/app/server.go`

- scheduler.New

在创建 scheduler 时最重要的就是注册了 Informer 的 ResourceEventHandler，也就是 `addAllEventHandlers`，当发现 Pod 出现类似 Add 事件时就会将其放入调度器的队列中。

```go
// New returns a Scheduler
func New(client clientset.Interface,
	informerFactory informers.SharedInformerFactory,
	recorderFactory profile.RecorderFactory,
	stopCh <-chan struct{},
	opts ...Option) (*Scheduler, error) {
	stopEverything := stopCh
	if stopEverything == nil {
		stopEverything = wait.NeverStop
	}

	// 默认的调度器配置
	options := defaultSchedulerOptions
	for _, opt := range opts {
		opt(&options)
	}

	if options.applyDefaultProfile {
		var versionedCfg v1beta2.KubeSchedulerConfiguration
		scheme.Scheme.Default(&versionedCfg)
		cfg := config.KubeSchedulerConfiguration{}
		if err := scheme.Scheme.Convert(&versionedCfg, &cfg, nil); err != nil {
			return nil, err
		}
		options.profiles = cfg.Profiles
	}
	schedulerCache := internalcache.New(30*time.Second, stopEverything)

	registry := frameworkplugins.NewInTreeRegistry()
	if err := registry.Merge(options.frameworkOutOfTreeRegistry); err != nil {
		return nil, err
	}

	snapshot := internalcache.NewEmptySnapshot()
	clusterEventMap := make(map[framework.ClusterEvent]sets.String)
	
    // 配置器，通过 configurator.create() 或 createFromPolicy 创建一个 Scheduler
	configurator := &Configurator{
		componentConfigVersion:   options.componentConfigVersion,
		client:                   client,
		kubeConfig:               options.kubeConfig,
		recorderFactory:          recorderFactory,
		informerFactory:          informerFactory,
		schedulerCache:           schedulerCache,
		StopEverything:           stopEverything,
		percentageOfNodesToScore: options.percentageOfNodesToScore,
		podInitialBackoffSeconds: options.podInitialBackoffSeconds,
		podMaxBackoffSeconds:     options.podMaxBackoffSeconds,
		profiles:                 append([]schedulerapi.KubeSchedulerProfile(nil), options.profiles...),
		registry:                 registry,
		nodeInfoSnapshot:         snapshot,
		extenders:                options.extenders,
		frameworkCapturer:        options.frameworkCapturer,
		parallellism:             options.parallelism,
		clusterEventMap:          clusterEventMap,
	}

	metrics.Register()

	var sched *Scheduler
	if options.legacyPolicySource == nil {
		sc, err := configurator.create()
		// ...
		sched = sc
	} else {
		// Create the config from a user specified policy source.
		policy := &schedulerapi.Policy{}
		switch {
		case options.legacyPolicySource.File != nil:
			if err := initPolicyFromFile(options.legacyPolicySource.File.Path, policy); err != nil {
				return nil, err
			}
		case options.legacyPolicySource.ConfigMap != nil:
			if err := initPolicyFromConfigMap(client, options.legacyPolicySource.ConfigMap, policy); err != nil {
				return nil, err
			}
		}
		configurator.extenders = policy.Extenders
		sc, err := configurator.createFromPolicy(*policy)
		if err != nil {
			return nil, fmt.Errorf("couldn't create scheduler from policy: %v", err)
		}
		sched = sc
	}

	// Additional tweaks to the config produced by the configurator.
	sched.StopEverything = stopEverything
	sched.client = client
	var dynInformerFactory dynamicinformer.DynamicSharedInformerFactory
	if options.kubeConfig != nil {
		dynClient := dynamic.NewForConfigOrDie(options.kubeConfig)
		dynInformerFactory = dynamicinformer.NewFilteredDynamicSharedInformerFactory(dynClient, 0, v1.NamespaceAll, nil)
	}

	// 启动一系列的资源对象监听对象，比如 Pod，有 Pod 事件发生时将其添加到调度器队列中
	addAllEventHandlers(sched, informerFactory, dynInformerFactory, unionedGVKs(clusterEventMap))
	return sched, nil
}
```

### 4.2 configurator.create

configurator.create 的主要逻辑 ：

1. 创建 profiles，内含 informer、kubeConfig、client、parallellism
2. 创建 LessFn，用于调度器队列内部 pod 优先级比较
3. 创建 SchedulingQueue
4. 创建 Scheduler

```go
// create a scheduler from a set of registered plugins.
func (c *Configurator) create() (*Scheduler, error) {
	// ...
	// api-server pod 的 Lister
	nominator := internalqueue.NewPodNominator(c.informerFactory.Core().V1().Pods().Lister())
    
    // 创建 profiles
	profiles, err := profile.NewMap(c.profiles, c.registry, c.recorderFactory,
		frameworkruntime.WithComponentConfigVersion(c.componentConfigVersion),
		frameworkruntime.WithClientSet(c.client),
		frameworkruntime.WithKubeConfig(c.kubeConfig),
		frameworkruntime.WithInformerFactory(c.informerFactory),
		frameworkruntime.WithSnapshotSharedLister(c.nodeInfoSnapshot),
		frameworkruntime.WithRunAllFilters(c.alwaysCheckAllPredicates),
		frameworkruntime.WithPodNominator(nominator),
		frameworkruntime.WithCaptureProfile(frameworkruntime.CaptureProfile(c.frameworkCapturer)),
		frameworkruntime.WithClusterEventMap(c.clusterEventMap),
		frameworkruntime.WithParallelism(int(c.parallellism)),
		frameworkruntime.WithExtenders(extenders),
	)
	
	// 交给 SchedulingQueue.activeQ 用于计算优先级
	lessFn := profiles[c.profiles[0].SchedulerName].QueueSortFunc()
	podQueue := internalqueue.NewSchedulingQueue(
		lessFn,
		c.informerFactory,
		internalqueue.WithPodInitialBackoffDuration(time.Duration(c.podInitialBackoffSeconds)*time.Second),
		internalqueue.WithPodMaxBackoffDuration(time.Duration(c.podMaxBackoffSeconds)*time.Second),
		internalqueue.WithPodNominator(nominator),
		internalqueue.WithClusterEventMap(c.clusterEventMap),
	)

	// Setup cache debugger.
	debugger := cachedebugger.New(
		c.informerFactory.Core().V1().Nodes().Lister(),
		c.informerFactory.Core().V1().Pods().Lister(),
		c.schedulerCache,
		podQueue,
	)
	debugger.ListenForSignal(c.StopEverything)

	algo := NewGenericScheduler(
		c.schedulerCache,
		c.nodeInfoSnapshot,
		c.percentageOfNodesToScore,
	)

	return &Scheduler{
		SchedulerCache:  c.schedulerCache,
		Algorithm:       algo,
		Extenders:       extenders,
		Profiles:        profiles,
		NextPod:         internalqueue.MakeNextPodFunc(podQueue),
		Error:           MakeDefaultErrorFunc(c.client, c.informerFactory.Core().V1().Pods().Lister(), podQueue, c.schedulerCache),
		StopEverything:  c.StopEverything,
		SchedulingQueue: podQueue,
	}, nil
}
```



### 4.3 addAllEventHandler 

addAllEventHandler 会 Watch api-server，将 Pod 的变化放入调度器队列，比如新增 Pod 时会执行 `sched.addPodToSchedulingQueue`

```go
// addAllEventHandlers is a helper function used in tests and in Scheduler
// to add event handlers for various informers.
func addAllEventHandlers(
	sched *Scheduler,
	informerFactory informers.SharedInformerFactory,
	dynInformerFactory dynamicinformer.DynamicSharedInformerFactory,
	gvkMap map[framework.GVK]framework.ActionType,
) {
	
	informerFactory.Core().V1().Pods().Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
			FilterFunc: func(obj interface{}) bool {
				switch t := obj.(type) {
				case *v1.Pod:
					return assignedPod(t)
				case cache.DeletedFinalStateUnknown:
					if pod, ok := t.Obj.(*v1.Pod); ok {
						return assignedPod(pod)
					}
					utilruntime.HandleError(fmt.Errorf("unable to convert object %T to *v1.Pod in %T", obj, sched))
					return false
				default:
					utilruntime.HandleError(fmt.Errorf("unable to handle object in %T: %T", sched, obj))
					return false
				}
			},
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToCache,
				UpdateFunc: sched.updatePodInCache,
				DeleteFunc: sched.deletePodFromCache,
			},
		},
	)
    // PodInformer 监听 Pod，有 Pod 资源变化时，通过 FilterFunc过滤
	// 如果 Pod 没有绑定到节点(未调度)，并且使用的是指定的调度器才可以进入 Handler 处理，比如 Pod Add 就会由 addPodToCache
	informerFactory.Core().V1().Pods().Informer().AddEventHandler(
		cache.FilteringResourceEventHandler{
            // 只有通过 FilterFunc 检查的事件才会执行下面的 Handler
			FilterFunc: func(obj interface{}) bool {
				switch t := obj.(type) {
				case *v1.Pod:
					return !assignedPod(t) && responsibleForPod(t, sched.Profiles)
				case cache.DeletedFinalStateUnknown:
					if pod, ok := t.Obj.(*v1.Pod); ok {
						return !assignedPod(pod) && responsibleForPod(pod, sched.Profiles)
					}
					utilruntime.HandleError(fmt.Errorf("unable to convert object %T to *v1.Pod in %T", obj, sched))
					return false
				default:
					utilruntime.HandleError(fmt.Errorf("unable to handle object in %T: %T", sched, obj))
					return false
				}
			},
            // 根据不同的事件进行处理
			Handler: cache.ResourceEventHandlerFuncs{
				AddFunc:    sched.addPodToSchedulingQueue,
				UpdateFunc: sched.updatePodInSchedulingQueue,
				DeleteFunc: sched.deletePodFromSchedulingQueue,
			},
		},
	)
	// 注册 node 监听器，Pod需要调度到 node 上，肯定得知道有哪些 node
	informerFactory.Core().V1().Nodes().Informer().AddEventHandler(
		cache.ResourceEventHandlerFuncs{
			AddFunc:    sched.addNodeToCache,
			UpdateFunc: sched.updateNodeInCache,
			DeleteFunc: sched.deleteNodeFromCache,
		},
	)
    
	// .....
}
```

addPodToSchedulingQueue 将 pod 放入调度队列的 activeQ ：

```go
func (sched *Scheduler) addPodToSchedulingQueue(obj interface{}) {
	pod := obj.(*v1.Pod)
	klog.V(3).InfoS("Add event for unscheduled pod", "pod", klog.KObj(pod))
	if err := sched.SchedulingQueue.Add(pod); err != nil {
		utilruntime.HandleError(fmt.Errorf("unable to queue %T: %v", obj, err))
	}
}

func (p *PriorityQueue) Add(pod *v1.Pod) error {
	p.lock.Lock()
	defer p.lock.Unlock()
	pInfo := p.newQueuedPodInfo(pod)
	if err := p.activeQ.Add(pInfo); err != nil {
		klog.ErrorS(err, "Error adding pod to the scheduling queue", "pod", klog.KObj(pod))
		return err
	}
	if p.unschedulableQ.get(pod) != nil {
		klog.ErrorS(nil, "Error: pod is already in the unschedulable queue", "pod", klog.KObj(pod))
		p.unschedulableQ.delete(pod)
	}
	// Delete pod from backoffQ if it is backing off
	if err := p.podBackoffQ.Delete(pInfo); err == nil {
		klog.ErrorS(nil, "Error: pod is already in the podBackoff queue", "pod", klog.KObj(pod))
	}
	metrics.SchedulerQueueIncomingPods.WithLabelValues("active", PodAdd).Inc()
	p.PodNominator.AddNominatedPod(pInfo.PodInfo, "")
	p.cond.Broadcast()

	return nil
}
```

## 5. 调度器启动

调度器启动 ：

- 启动调度队列
- scheduleOne 开始从调度队列中获取数据

```go
// pkg/scheduler/scheduler.go
// Run函数开始监听和调度
func (sched *Scheduler) Run(ctx context.Context) {
	sched.SchedulingQueue.Run()
	wait.UntilWithContext(ctx, sched.scheduleOne, 0)
	sched.SchedulingQueue.Close()
}
```

核心是 `sched.scheduleOne` ：这部分代码很长，我省略了部分不重要的 error 处理和日志打印代码。

scheduleOne 包含了调度周期和绑定周期 ：

1. 调用 sched.NextPod 从 activeQ中 获取一个优先级最高的待调度 Pod，该过程是阻塞的，当 activeQ 中不存在任何 Pod 资源对象时，sched.NextPod 处于等待状态
2. 调用 sched.Algorithm.Schedule 方法执行预选调度算法和优选调度算法，为 Pod 选择一个合适的节点
3. 调用 sched.assume 方法进行预绑定，为 Pod 设置 NodeName 字段，更新 Scheduler 缓存
4. 调用 fwk.RunReservePluginsReserve 方法运行 Reserve 插件的 Reserve 方法
5. 调用 fwk.RunPermitPlugins 方法运行 Permit 插件
6. 调用 fwk.RunPreBindPlugins 方法运行 PreBind 插件
7. 调用 sched.bind 方法进行真正的绑定，请求 ApiServer 异步处理最终的绑定操作，写入 etcd
8. 绑定成功后，调用 fwk.RunPostBindPlugins 方法运行 PostBind 插件

```go
// scheduleOne does the entire scheduling workflow for a single pod. It is serialized on the scheduling algorithm's host fitting.
func (sched *Scheduler) scheduleOne(ctx context.Context) {
	// 1. 阻塞获取一个待调度的 Pod （优先级最高）
	// 执行的是 internalqueue.MakeNextPodFunc(podQueue)
	podInfo := sched.NextPod()
	if podInfo == nil || podInfo.Pod == nil {
		return
	}
	pod := podInfo.Pod
	fwk, err := sched.frameworkForPod(pod)
	
	// 2. 是否要跳过 Pod 的调度
	if sched.skipPodSchedule(fwk, pod) {
		return
	}
    
	// Synchronously attempt to find a fit for the pod.
	start := time.Now()
	state := framework.NewCycleState()
	state.SetRecordPluginMetrics(rand.Intn(100) < pluginMetricsSamplePercent)

	podsToActivate := framework.NewPodsToActivate()
	state.Write(framework.PodsToActivateKey, podsToActivate)

	schedulingCycleCtx, cancel := context.WithCancel(ctx)
	defer cancel()
	// 3. 执行预选与优选算法的处理
	scheduleResult, err := sched.Algorithm.Schedule(schedulingCycleCtx, sched.Extenders, fwk, state, pod)
	// 调度失败: 尝试抢占低优先级pod的 node
	if err != nil {
		nominatedNode := ""
		if fitError, ok := err.(*framework.FitError); ok {
			// 执行所有的 PostFilter
			if !fwk.HasPostFilterPlugins() {
				//...
			} else {
				// 抢占逻辑，返回被抢占的 node 节点
				// PostFilter 只有一个 DefaultPreemption 步骤
				result, status := fwk.RunPostFilterPlugins(ctx, state, pod, fitError.Diagnosis.NodeToStatusMap)
				if status.IsSuccess() && result != nil {
					nominatedNode = result.NominatedNodeName
				}
			}
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
		} else if err == ErrNoNodesAvailable {
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
		} else {
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
		}
		// 抢占结束后记录日志，将被抢占的pod放入调度失败队列中
		sched.recordSchedulingFailure(fwk, podInfo, err, v1.PodReasonUnschedulable, nominatedNode)
		return
	}
	metrics.SchedulingAlgorithmLatency.Observe(metrics.SinceInSeconds(start))
	assumedPodInfo := podInfo.DeepCopy()
	assumedPod := assumedPodInfo.Pod
	// 4. 预绑定，为 Pod 设置 NodeName
	err = sched.assume(assumedPod, scheduleResult.SuggestedHost)

	if err != nil {
		metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
		sched.recordSchedulingFailure(fwk, assumedPodInfo, err, SchedulerError, "")
		return
	}

	// 5. 运行 Reserve 插件的 Reserve 方法
	if sts := fwk.RunReservePluginsReserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost); !sts.IsSuccess() {
		metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
		fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		if forgetErr := sched.SchedulerCache.ForgetPod(assumedPod); forgetErr != nil {
			// ...
		}
		sched.recordSchedulingFailure(fwk, assumedPodInfo, sts.AsError(), SchedulerError, "")
		return
	}

	// 运行 Permit 插件
	runPermitStatus := fwk.RunPermitPlugins(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
	if runPermitStatus.Code() != framework.Wait && !runPermitStatus.IsSuccess() {
		var reason string
		if runPermitStatus.IsUnschedulable() {
			metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
			reason = v1.PodReasonUnschedulable
		} else {
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
			reason = SchedulerError
		}
		fwk.RunReservePluginsUnreserve(schedulingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		if forgetErr := sched.SchedulerCache.ForgetPod(assumedPod); forgetErr != nil {
			klog.ErrorS(forgetErr, "scheduler cache ForgetPod failed")
		}
		sched.recordSchedulingFailure(fwk, assumedPodInfo, runPermitStatus.AsError(), reason, "")
		return
	}

	if len(podsToActivate.Map) != 0 {
		sched.SchedulingQueue.Activate(podsToActivate.Map)
		podsToActivate.Map = make(map[string]*v1.Pod)
	}

	// 绑定阶段是异步的
	go func() {
		bindingCycleCtx, cancel := context.WithCancel(ctx)
		defer cancel()
		metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Inc()
		defer metrics.SchedulerGoroutines.WithLabelValues(metrics.Binding).Dec()

		waitOnPermitStatus := fwk.WaitOnPermit(bindingCycleCtx, assumedPod)
		if !waitOnPermitStatus.IsSuccess() {
			var reason string
			if waitOnPermitStatus.IsUnschedulable() {
				metrics.PodUnschedulable(fwk.ProfileName(), metrics.SinceInSeconds(start))
				reason = v1.PodReasonUnschedulable
			} else {
				metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
				reason = SchedulerError
			}
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			if forgetErr := sched.SchedulerCache.ForgetPod(assumedPod); forgetErr != nil {
				klog.ErrorS(forgetErr, "scheduler cache ForgetPod failed")
			}
			sched.recordSchedulingFailure(fwk, assumedPodInfo, waitOnPermitStatus.AsError(), reason, "")
			return
		}

		// 预绑定插件
		preBindStatus := fwk.RunPreBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
		if !preBindStatus.IsSuccess() {
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			if forgetErr := sched.SchedulerCache.ForgetPod(assumedPod); forgetErr != nil {
				klog.ErrorS(forgetErr, "scheduler cache ForgetPod failed")
			}
			sched.recordSchedulingFailure(fwk, assumedPodInfo, preBindStatus.AsError(), SchedulerError, "")
			return
		}

		// 真正绑定
		err := sched.bind(bindingCycleCtx, fwk, assumedPod, scheduleResult.SuggestedHost, state)
		if err != nil {
			metrics.PodScheduleError(fwk.ProfileName(), metrics.SinceInSeconds(start))
			fwk.RunReservePluginsUnreserve(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)
			if err := sched.SchedulerCache.ForgetPod(assumedPod); err != nil {
                // ...
			}
			sched.recordSchedulingFailure(fwk, assumedPodInfo, fmt.Errorf("binding rejected: %w", err), SchedulerError, "")
		} else {
			metrics.PodScheduled(fwk.ProfileName(), metrics.SinceInSeconds(start))
			metrics.PodSchedulingAttempts.Observe(float64(podInfo.Attempts))
			metrics.PodSchedulingDuration.WithLabelValues(getAttemptsLabel(podInfo)).Observe(metrics.SinceInSeconds(podInfo.InitialAttemptTimestamp))

			// Run "postbind" plugins.
			fwk.RunPostBindPlugins(bindingCycleCtx, state, assumedPod, scheduleResult.SuggestedHost)

			// At the end of a successful binding cycle, move up Pods if needed.
			if len(podsToActivate.Map) != 0 {
				sched.SchedulingQueue.Activate(podsToActivate.Map)
			}
		}
	}()
}
```

### 5.1 NextPod

在创建 Scheduler 时会赋值 `NextPod` 为 `MakeNextPodFunc`，该方法会阻塞等待 activeQ 中出现数据。

```go
func MakeNextPodFunc(queue SchedulingQueue) func() *framework.QueuedPodInfo {
	return func() *framework.QueuedPodInfo {
		podInfo, err := queue.Pop()
		if err == nil {
			klog.V(4).InfoS("About to try and schedule pod", "pod", klog.KObj(podInfo.Pod))
			return podInfo
		}
		klog.ErrorS(err, "Error while retrieving next pod from scheduling queue")
		return nil
	}
}
```

```go
func (p *PriorityQueue) Pop() (*framework.QueuedPodInfo, error) {
	p.lock.Lock()
	defer p.lock.Unlock()
	for p.activeQ.Len() == 0 {
		if p.closed {
			return nil, fmt.Errorf(queueClosed)
		}
		p.cond.Wait()
	}
	obj, err := p.activeQ.Pop()
	if err != nil {
		return nil, err
	}
	pInfo := obj.(*framework.QueuedPodInfo)
	pInfo.Attempts++
	p.schedulingCycle++
	return pInfo, err
}
```

### 5.2 Schedule

`scheduleOne` 通过 `Schedule` 方法来执行预选与优选算法的处理

1. 获取 node 的快照信息，每一次调度都要获取
2. 找到所有满足条件的 nodes （预选）
    - 没有满足条件的 nodes ：直接返回
    - 只有一个满足条件的 node ：就用它
    - 有多个满足条件的 nodes ：使用 `prioritizeNodes` 给这些 node 打分，选取最高分调度（优选）

```go
func (g *genericScheduler) Schedule(ctx context.Context, extenders []framework.Extender, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) (result ScheduleResult, err error) {
	trace := utiltrace.New("Scheduling", utiltrace.Field{Key: "namespace", Value: pod.Namespace}, utiltrace.Field{Key: "name", Value: pod.Name})
	defer trace.LogIfLong(100 * time.Millisecond)

	// 1. 获取 node 的快照信息，每一次调度时都要获取
	if err := g.snapshot(); err != nil {
		return result, err
	}
	trace.Step("Snapshotting scheduler cache and node infos done")

	if g.nodeInfoSnapshot.NumNodes() == 0 {
		return result, ErrNoNodesAvailable
	}

	// 2. Predicates 阶段：找到所有满足调度条件的节点
	feasibleNodes, diagnosis, err := g.findNodesThatFitPod(ctx, extenders, fwk, state, pod)
	if err != nil {
		return result, err
	}
	trace.Step("Computing predicates done")

	// 3. 没有合适的 node，直接返回
	if len(feasibleNodes) == 0 {
		return result, &framework.FitError{
			Pod:         pod,
			NumAllNodes: g.nodeInfoSnapshot.NumNodes(),
			Diagnosis:   diagnosis,
		}
	}

	// When only one node after predicate, just use it.
	// 4. 只有一个合适的节点，那就用它
	if len(feasibleNodes) == 1 {
		return ScheduleResult{
			SuggestedHost:  feasibleNodes[0].Name,
			EvaluatedNodes: 1 + len(diagnosis.NodeToStatusMap),
			FeasibleNodes:  1,
		}, nil
	}

	// 5. 有很多合适的节点，执行优选算法
	// 运行所有打分插件对节点打分，返回一个优先级队列
	priorityList, err := prioritizeNodes(ctx, extenders, fwk, state, pod, feasibleNodes)
	if err != nil {
		return result, err
	}

	// 6. 选择一个分数最高的
	host, err := g.selectHost(priorityList)
	trace.Step("Prioritizing done")

	return ScheduleResult{
		SuggestedHost:  host,
		EvaluatedNodes: len(feasibleNodes) + len(diagnosis.NodeToStatusMap),
		FeasibleNodes:  len(feasibleNodes),
	}, err
}
```

#### 5.2.1 预选 findNodesThatFitPod

1. 调用 fwk.RunPreFilterPlugins 方法，运行 prefilter 插件，预处理
2. 调用 findNodesThatPassFilters 方法，查找能够满足 filter 过滤插件的节点
3. 调用 findNodesThatPassExtenders 方法，查找能够满足 extenders 的节点

```go
func (g *genericScheduler) findNodesThatFitPod(ctx context.Context, extenders []framework.Extender, fwk framework.Framework, state *framework.CycleState, pod *v1.Pod) ([]*v1.Node, framework.Diagnosis, error) {
	diagnosis := framework.Diagnosis{
		NodeToStatusMap:      make(framework.NodeToStatusMap),
		UnschedulablePlugins: sets.NewString(),
	}

	// 运行 prefilter 插件
	s := fwk.RunPreFilterPlugins(ctx, state, pod)
	allNodes, err := g.nodeInfoSnapshot.NodeInfos().List()
	if err != nil {
		return nil, diagnosis, err
	}
	if !s.IsSuccess() {
		if !s.IsUnschedulable() {
			return nil, diagnosis, s.AsError()
		}
		for _, n := range allNodes {
			diagnosis.NodeToStatusMap[n.Node().Name] = s
		}
		diagnosis.UnschedulablePlugins.Insert(s.FailedPlugin())
		return nil, diagnosis, nil
	}

	if len(pod.Status.NominatedNodeName) > 0 && feature.DefaultFeatureGate.Enabled(features.PreferNominatedNode) {
		feasibleNodes, err := g.evaluateNominatedNode(ctx, extenders, pod, fwk, state, diagnosis)
		if err != nil {
			klog.ErrorS(err, "Evaluation failed on nominated node", "pod", klog.KObj(pod), "node", pod.Status.NominatedNodeName)
		}
		if len(feasibleNodes) != 0 {
			return feasibleNodes, diagnosis, nil
		}
	}
	// 拿到所有通过了过滤器的节点
	feasibleNodes, err := g.findNodesThatPassFilters(ctx, fwk, state, pod, diagnosis, allNodes)
	if err != nil {
		return nil, diagnosis, err
	}

	// 拿到所有通过了扩展器过滤器的节点
	feasibleNodes, err = findNodesThatPassExtenders(extenders, pod, feasibleNodes, diagnosis.NodeToStatusMap)
	if err != nil {
		return nil, diagnosis, err
	}
	return feasibleNodes, diagnosis, nil
}
```



#### 5.2.2 优选 prioritizeNodes

1. 没有打分插件则给所有 node 的 score 赋值为 1
2. 运行 `PreScore` 打分插件
3. 运行 `Score` 打分插件

```go
func prioritizeNodes(
	ctx context.Context,
	extenders []framework.Extender,
	fwk framework.Framework,
	state *framework.CycleState,
	pod *v1.Pod,
	nodes []*v1.Node,
) (framework.NodeScoreList, error) {
	// 没有打分插件则给所有 node 的 score 赋值为 1
	if len(extenders) == 0 && !fwk.HasScorePlugins() {
		result := make(framework.NodeScoreList, 0, len(nodes))
		for i := range nodes {
			result = append(result, framework.NodeScore{
				Name:  nodes[i].Name,
				Score: 1,
			})
		}
		return result, nil
	}

	// Run PreScore plugins.
	preScoreStatus := fwk.RunPreScorePlugins(ctx, state, pod, nodes)
	if !preScoreStatus.IsSuccess() {
		return nil, preScoreStatus.AsError()
	}

	// Run the Score plugins.
	scoresMap, scoreStatus := fwk.RunScorePlugins(ctx, state, pod, nodes)
	if !scoreStatus.IsSuccess() {
		return nil, scoreStatus.AsError()
	}

	// Summarize all scores.
	result := make(framework.NodeScoreList, 0, len(nodes))

	for i := range nodes {
		result = append(result, framework.NodeScore{Name: nodes[i].Name, Score: 0})
		for j := range scoresMap {
			result[i].Score += scoresMap[j][i].Score
		}
	}

    // 如果配置了 extender, 还要调用 extender 对 node 评分并累加到 result 中
    // 因为要多协程并发调用 extender 并统计分数, 所以需要锁来互斥写入 node 分数
	if len(extenders) != 0 && nodes != nil {
		var mu sync.Mutex
		var wg sync.WaitGroup
		combinedScores := make(map[string]int64, len(nodes))
		for i := range extenders {
			if !extenders[i].IsInterested(pod) {
				continue
			}
			wg.Add(1)
			go func(extIndex int) {
				metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Inc()
				defer func() {
					metrics.SchedulerGoroutines.WithLabelValues(metrics.PrioritizingExtender).Dec()
					wg.Done()
				}()
				prioritizedList, weight, err := extenders[extIndex].Prioritize(pod, nodes)
				mu.Lock()
				for i := range *prioritizedList {
					host, score := (*prioritizedList)[i].Host, (*prioritizedList)[i].Score
					
					combinedScores[host] += score * weight
				}
				mu.Unlock()
			}(i)
		}
		// 等待统计分数
		wg.Wait()
		for i := range result {
			result[i].Score += combinedScores[result[i].Name] * (framework.MaxNodeScore / extenderv1.MaxExtenderPriority)
		}
	}

	return result, nil
}
```

### 5.3 assume

预绑定，更新缓存，为 Pod 设置 NodeName

```go
// 预绑定，为 Pod 设置 NodeName, 更新 Scheduler 缓存
func (sched *Scheduler) assume(assumed *v1.Pod, host string) error {
	assumed.Spec.NodeName = host
	// 将 SchedulerCache 中关于该 pod 的缓存更新
	if err := sched.SchedulerCache.AssumePod(assumed); err != nil {
		klog.ErrorS(err, "scheduler cache AssumePod failed")
		return err
	}
	// if "assumed" is a nominated pod, we should remove it from internal cache
	if sched.SchedulingQueue != nil {
		sched.SchedulingQueue.DeleteNominatedPodIfExists(assumed)
	}

	return nil
}
```

### 5.4 Bind

调用的是 defaultBinder，发送请求给 api-server 进行绑定。

```go
// Bind binds pods to nodes using the k8s client.
func (b DefaultBinder) Bind(ctx context.Context, state *framework.CycleState, p *v1.Pod, nodeName string) *framework.Status {
	klog.V(3).InfoS("Attempting to bind pod to node", "pod", klog.KObj(p), "node", nodeName)
	binding := &v1.Binding{
		ObjectMeta: metav1.ObjectMeta{Namespace: p.Namespace, Name: p.Name, UID: p.UID},
		Target:     v1.ObjectReference{Kind: "Node", Name: nodeName},
	}
	err := b.handle.ClientSet().CoreV1().Pods(binding.Namespace).Bind(ctx, binding, metav1.CreateOptions{})
	if err != nil {
		return framework.AsStatus(err)
	}
	return nil
}
```

kubelet 会监听 api-server 中绑定到该 node 的 pod，拉取后run  

## 6. 总结

1. 通过 Informer 获取需要调度的 Pod 放入调度队列
2. 从调度队列中获取数据走完 调度周期、绑定周期
